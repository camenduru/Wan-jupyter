{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/Wan2.1-jupyter/blob/main/Wan2.1_T2I_jupyter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\n",
    "!git clone https://github.com/city96/ComfyUI-GGUF /content/ComfyUI/custom_nodes/ComfyUI-GGUF\n",
    "!pip install torchsde gguf\n",
    "\n",
    "!apt install aria2 -qqy\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/city96/Wan2.1-T2V-14B-gguf/resolve/main/wan2.1-t2v-14b-Q3_K_S.gguf -d /content/ComfyUI/models/unet -o wan2.1-t2v-14b-Q3_K_S.gguf\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/city96/umt5-xxl-encoder-gguf/resolve/main/umt5-xxl-encoder-Q3_K_S.gguf -d /content/ComfyUI/models/clip -o umt5-xxl-encoder-Q3_K_S.gguf\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors -d /content/ComfyUI/models/vae -o/wan_2.1_vae.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/FusionX_LoRa/Wan2.1_T2V_14B_FusionX_LoRA.safetensors -d /content/ComfyUI/models/loras/FusionX -o Wan2.1_T2V_14B_FusionX_LoRA.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/ComfyUI\n",
    "\n",
    "import torch\n",
    "import random, time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from nodes import NODE_CLASS_MAPPINGS, load_custom_node\n",
    "from comfy_extras import nodes_hunyuan, nodes_model_advanced\n",
    "\n",
    "load_custom_node(\"/content/ComfyUI/custom_nodes/ComfyUI-GGUF\")\n",
    "\n",
    "UnetLoaderGGUF = NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
    "CLIPLoaderGGUF = NODE_CLASS_MAPPINGS[\"CLIPLoaderGGUF\"]()\n",
    "LoraLoaderModelOnly = NODE_CLASS_MAPPINGS[\"LoraLoaderModelOnly\"]()\n",
    "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "\n",
    "CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "EmptyHunyuanLatentVideo = nodes_hunyuan.NODE_CLASS_MAPPINGS[\"EmptyHunyuanLatentVideo\"]()\n",
    "\n",
    "KSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
    "ModelSamplingSD3 = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingSD3\"]()\n",
    "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    unet = UnetLoaderGGUF.load_unet(\"wan2.1-t2v-14b-Q3_K_S.gguf\")[0]\n",
    "    clip = CLIPLoaderGGUF.load_clip(\"umt5-xxl-encoder-Q3_K_S.gguf\", \"wan\")[0]\n",
    "    lora = LoraLoaderModelOnly.load_lora_model_only(unet, \"FusionX/Wan2.1_T2V_14B_FusionX_LoRA.safetensors\", 1.0)[0]\n",
    "    vae = VAELoader.load_vae(\"wan_2.1_vae.safetensors\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    seed = 0\n",
    "    steps = 10\n",
    "    cfg = 1.0\n",
    "    sampler_name = \"euler\"\n",
    "    scheduler = \"beta\"\n",
    "    width = 1280\n",
    "    height = 720\n",
    "    length = 1\n",
    "    positive_prompt = \"In a dim, candlelit medieval hall, a queen walks slowly along a corridor lined with flickering torches. The camera tracks her from behind in a long, steady tracking shot, capturing the heavy drape of her velvet gown and the echo of her footsteps. Shadows dance across the stone walls, and dust swirls in the warm volumetric light. The mood is regal, tense, and quietly suspenseful.\"\n",
    "    negative_prompt = \"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走\"\n",
    "    positive = CLIPTextEncode.encode(clip, positive_prompt)[0]\n",
    "    negative = CLIPTextEncode.encode(clip, negative_prompt)[0]\n",
    "    model = ModelSamplingSD3.patch(lora, 1.0)[0]\n",
    "    latent_image = EmptyHunyuanLatentVideo.generate(width, height, length)[0]\n",
    "    if seed == 0:\n",
    "        random.seed(int(time.time()))\n",
    "        seed = random.randint(0, 18446744073709551615)\n",
    "    samples = KSampler.sample(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image)[0]\n",
    "\n",
    "    decoded = VAEDecode.decode(vae, samples)[0].detach()\n",
    "    image = Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(f\"/content/test.png\")\n",
    "\n",
    "Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
